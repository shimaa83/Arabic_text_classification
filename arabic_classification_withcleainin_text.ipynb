{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb306d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Tashaphyne in c:\\users\\ma\\anaconda3\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: pyarabic in c:\\users\\ma\\anaconda3\\lib\\site-packages (from Tashaphyne) (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\ma\\anaconda3\\lib\\site-packages (from pyarabic->Tashaphyne) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "!pip install Tashaphyne\n",
    "import pyarabic.arabrepr\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ea079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  Positive  ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...\n",
       "1  Positive  أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...\n",
       "2  Positive  هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...\n",
       "3  Positive  خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...\n",
       "4  Positive  ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:/NLP/arabic data set/archive/ar_reviews_3.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7a9c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =['من',\n",
    " 'في',\n",
    " 'على',\n",
    " 'و',\n",
    " 'فى',\n",
    " 'يا',\n",
    " 'عن',\n",
    " 'مع',\n",
    " 'ان',\n",
    " 'هو',\n",
    " 'علي',\n",
    " 'ما',\n",
    " 'اللي',\n",
    " 'كل',\n",
    " 'بعد',\n",
    " 'ده',\n",
    " 'اليوم',\n",
    " 'أن',\n",
    " 'يوم',\n",
    " 'انا',\n",
    " 'إلى',\n",
    " 'كان',\n",
    " 'ايه',\n",
    " 'اللى',\n",
    " 'الى',\n",
    " 'دي',\n",
    " 'بين',\n",
    " 'انت',\n",
    " 'أنا',\n",
    " 'حتى',\n",
    " 'لما',\n",
    " 'فيه',\n",
    " 'هذا',\n",
    " 'واحد',\n",
    " 'احنا',\n",
    " 'اي',\n",
    " 'كده',\n",
    " 'إن',\n",
    " 'او',\n",
    " 'أو',\n",
    " 'عليه',\n",
    " 'ف',\n",
    " 'دى',\n",
    " 'مين',\n",
    " 'الي',\n",
    " 'كانت',\n",
    " 'أمام',\n",
    " 'زي',\n",
    " 'يكون',\n",
    " 'خلال',\n",
    " 'ع',\n",
    " 'كنت',\n",
    " 'هي',\n",
    " 'فيها',\n",
    " 'عند',\n",
    " 'التي',\n",
    " 'الذي',\n",
    " 'قال',\n",
    " 'هذه',\n",
    " 'قد',\n",
    " 'انه',\n",
    " 'ريتويت',\n",
    " 'بعض',\n",
    " 'أول',\n",
    " 'ايه',\n",
    " 'الان',\n",
    " 'أي',\n",
    " 'منذ',\n",
    " 'عليها',\n",
    " 'له',\n",
    " 'ال',\n",
    " 'تم',\n",
    " 'ب',\n",
    " 'دة',\n",
    " 'عليك',\n",
    " 'اى',\n",
    " 'كلها',\n",
    " 'اللتى',\n",
    " 'هى',\n",
    " 'دا',\n",
    " 'انك',\n",
    " 'وهو',\n",
    " 'ومن',\n",
    " 'منك',\n",
    " 'نحن',\n",
    " 'زى',\n",
    " 'أنت',\n",
    " 'انهم',\n",
    " 'معانا',\n",
    " 'حتي',\n",
    " 'وانا',\n",
    " 'عنه',\n",
    " 'إلي',\n",
    " 'ونحن',\n",
    " 'وانت',\n",
    " 'منكم',\n",
    " 'وان',\n",
    " 'معاهم',\n",
    " 'معايا',\n",
    " 'وأنا',\n",
    " 'عنها',\n",
    " 'إنه',\n",
    " 'اني',\n",
    " 'معك',\n",
    " 'اننا',\n",
    " 'فيهم',\n",
    " 'د',\n",
    " 'انتا',\n",
    " 'عنك',\n",
    " 'وهى',\n",
    " 'معا',\n",
    " 'آن',\n",
    " 'انتي',\n",
    " 'وأنت',\n",
    " 'وإن',\n",
    " 'ومع',\n",
    " 'وعن',\n",
    " 'معاكم',\n",
    " 'معاكو',\n",
    " 'معاها',\n",
    " 'وعليه',\n",
    " 'وانتم',\n",
    " 'وانتي',\n",
    " '¿',\n",
    " '|']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276df049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    return:\n",
    "        string of words but standardize the words\n",
    "    '''\n",
    "    sentence = re.sub(\"[إأآا]\", \"ا\", sentence)\n",
    "    sentence = re.sub(\"ى\", \"ي\", sentence)\n",
    "    sentence = re.sub(\"ؤ\", \"ء\", sentence)\n",
    "    sentence = re.sub(\"ئ\", \"ء\", sentence)\n",
    "    sentence = re.sub(\"ة\", \"ه\", sentence)\n",
    "    sentence = re.sub(\"گ\", \"ك\", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c032074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_ar_stopwords(text):\n",
    "    \"\"\"\n",
    "        Here we remove all Arabic stop words\n",
    "        \n",
    "    \"\"\"\n",
    "      # if read it from file\n",
    "#     ar_stopwords_list = open(\"arabic_stopwords.txt\", \"r\") \n",
    "#     stop_words = ar_stopwords_list.read().split(\"\\n\")\n",
    "#     stop_words = []\n",
    "    original_words = []\n",
    "    words = word_tokenize(text) # it works on one sentence not list\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            original_words.append(word)\n",
    "    filtered_sentence = \" \".join(original_words)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20331308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearReg(text):\n",
    "    \"\"\"\n",
    "        This function for getting the normal values of out of lemmatization function\n",
    "        that takse a string of dict as a \n",
    "        takes  : '{\"result\":[\"امر\",\"ب\",\"أخذ\",\"ما\",\"نهى\",\"ه\",\"انتهى\"]}'\n",
    "        return : ['امر أخذ ما نهى انتهى']\n",
    "    \"\"\"\n",
    "    each_lemma_word = []\n",
    "    each_lemma_sentence = []\n",
    "    for final_data in text:\n",
    "        matches = re.findall(r'\\\"(.+?)\\\"',final_data)\n",
    "        for word in matches:\n",
    "            if len(word) >= 2 and word !='result':\n",
    "                each_lemma_word.append(word)\n",
    "        each_lemma_sentence.append(\" \".join(each_lemma_word))\n",
    "        each_lemma_word.clear()\n",
    "    return each_lemma_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "351fa436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming_2 by Tashaphyne is an Arabic light stemmer(removing prefixes and suffixes) and give all possible segmentations.\n",
    "#Stemming : algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word.\n",
    "\n",
    "def stemming_2(text):\n",
    "    \"\"\"\n",
    "        This is  functoin for stemming and it's looks good results, with built in library called Tashaphyne.\n",
    "        The documentation here ==> https://pypi.org/project/Tashaphyne/\n",
    "    \n",
    "    \"\"\"\n",
    "    import pyarabic.arabrepr\n",
    "    arepr = pyarabic.arabrepr.ArabicRepr()\n",
    "    repr = arepr.repr\n",
    "\n",
    "    from tashaphyne.stemming import ArabicLightStemmer\n",
    "    ArListem = ArabicLightStemmer()\n",
    "\n",
    "    final_data_without_stop_words_and_with_normalization_and_with_stemming = []\n",
    "\n",
    "    for final_data in final_data_without_stop_words_and_with_normalization:\n",
    "        words = word_tokenize(final_data)\n",
    "        new_list = []\n",
    "        for word in words:\n",
    "            stem = ArListem.light_stem(word)\n",
    "            stem = ArListem.get_stem()\n",
    "            new_list.append(stem)\n",
    "\n",
    "        final_data_sentence_with_stemming = \" \".join(new_list)\n",
    "        final_data_without_stop_words_and_with_normalization_and_with_stemming.append(final_data_sentence_with_stemming)\n",
    "        \n",
    "    return final_data_without_stop_words_and_with_normalization_and_with_stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb4b371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization by Farasa API\n",
    "#Lemmatization : takes into consideration the morphological analysis of the words.\n",
    "\n",
    "def lemmatization(text):\n",
    "    \"\"\"\n",
    "        This function for lemma Arabic words by API, and it getting best result of the previous functions\n",
    "        return a string dictinary like exactly '{\"result\":[\"امر\",\"ب\",\"أخذ\",\"ما\",\"نهى\",\"ه\",\"انتهى\"]}'\n",
    "    \"\"\"\n",
    "    import http.client\n",
    "    conn = http.client.HTTPSConnection(\"farasa-api.qcri.org\")\n",
    "    final_data_dict = {}\n",
    "    list_pyload_input = []\n",
    "    list_pyload_out = []\n",
    "    length = len(text)\n",
    "    for h in text[:length]:\n",
    "        q = '{\"text\":'+'\"{}\"'.format(h)+'}'\n",
    "        list_pyload_input.append(q)\n",
    "    headers = { 'content-type': \"application/json\", 'cache-control': \"no-cache\", }\n",
    "    for h in list_pyload_input:\n",
    "        conn.request(\"POST\", \"/msa/webapi/lemma\", h.encode('utf-8'), headers)\n",
    "        res = conn.getresponse()\n",
    "        data = res.read()\n",
    "        list_pyload_out.append(data.decode(\"utf-8\"))\n",
    "        final_result = clearReg(list_pyload_out)     # call clearReg for clean the text\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a5be992",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= []\n",
    "\n",
    "for words in df['text']:\n",
    "    data.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ccabb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# sentiment analysis first step which is cleanig data\n",
    "cleared_data_1 = []           # Removing stopwords\n",
    "cleared_data_1_2 = []         # Normalization\n",
    "#cleared_data_1_2_3 = []       # Lematization\n",
    "\n",
    "for final_data in data:\n",
    "    cleared_data_1.append(removing_ar_stopwords(final_data))         # Removing stopwords\n",
    "len(cleared_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f89399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for final_data in cleared_data_1:\n",
    "    cleared_data_1_2.append(normalize(final_data))                   # Normalization\n",
    "    \n",
    "len(cleared_data_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01346eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleared_data_1_2_3 = lemmatization(cleared_data_1_2)           # Lematization\n",
    "\n",
    "#print('The size of data:')\n",
    "#len(cleared_data_1), len(cleared_data_1_2), len(cleared_data_1_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a97a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_data'] = cleared_data_1_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e582e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('D:/NLP/arabic data set/archive/file.tsv',sep=\"\\t\",encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50a2917a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...</td>\n",
       "      <td>ممتاز نوعا . النظافه والموقع والتجهيز والشاطيء...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...</td>\n",
       "      <td>احد اسباب نجاح الامارات شخص الدوله يعشق ترابها...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...</td>\n",
       "      <td>هادفه .. وقويه . تنقلك صخب شوارع القاهره هدوء ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...</td>\n",
       "      <td>خلصنا .. مبدءيا مستني ابهار الفيل الازرق ميقرا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...</td>\n",
       "      <td>ياسات جلوريا جزء لا يتجزا دبي . فندق متكامل ال...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0  Positive  ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...   \n",
       "1  Positive  أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...   \n",
       "2  Positive  هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...   \n",
       "3  Positive  خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...   \n",
       "4  Positive  ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...   \n",
       "\n",
       "                                          clean_data  \n",
       "0  ممتاز نوعا . النظافه والموقع والتجهيز والشاطيء...  \n",
       "1  احد اسباب نجاح الامارات شخص الدوله يعشق ترابها...  \n",
       "2  هادفه .. وقويه . تنقلك صخب شوارع القاهره هدوء ...  \n",
       "3  خلصنا .. مبدءيا مستني ابهار الفيل الازرق ميقرا...  \n",
       "4  ياسات جلوريا جزء لا يتجزا دبي . فندق متكامل ال...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7e1bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_data']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eebfe138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (40199,)\n",
      "Testing Data Shape:  (19800,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Training Data Shape:', X_train.shape)\n",
    "print('Testing Data Shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a5c826b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mixed       13425\n",
       "Positive    13399\n",
       "Negative    13375\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb821fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40199, 164029)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5f75f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),])\n",
    "\n",
    "\n",
    "text_clf.fit(X_train, y_train)  \n",
    "\n",
    "\n",
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfc25806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3470 1447 1658]\n",
      " [1296 4694  635]\n",
      " [1308  648 4644]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdbee79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Mixed       0.57      0.53      0.55      6575\n",
      "    Negative       0.69      0.71      0.70      6625\n",
      "    Positive       0.67      0.70      0.69      6600\n",
      "\n",
      "    accuracy                           0.65     19800\n",
      "   macro avg       0.64      0.65      0.64     19800\n",
      "weighted avg       0.64      0.65      0.65     19800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a111435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
